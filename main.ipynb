{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e919468a",
   "metadata": {},
   "source": [
    "# Local Retrieval Augmented Generation (RAG) Chatbot\n",
    "\n",
    "**Auther** Venkat Krishna Madabooshini  \n",
    "**Description:** This notebook demonstrates a RAG - powered local chatbot capabale of answering queries on PDF/text documents using LangChain, ChromaDB, Ollama and LLM model of choice. Chatbot also supports persistent converssation memory for seamless multi-turn interactions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2efc9a",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation  \n",
    "\n",
    "The following code installs necessary packages (run only if not already installed).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#follow thse steps to create an isolated python environment and install all dependencies\n",
    "# create a virtual envrionment\n",
    "# python -m venv venv\n",
    "# activate the virtual environment\n",
    "# On windows (PowerShell):\n",
    "# venv\\Scripts\\Activate.ps1\n",
    "# On macOS/Linux:\n",
    "# source venv/bin/activate\n",
    "# pip install langchain chromadb langchain-community \"unstructured[pdf]\" langchain-chroma langchain-ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be33ec",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a39db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries for loading in the text document and spliting the data into chunks\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "#from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#Libraries for creating the (chromadb) vector database, text embeddings \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "#Libraries for creating retrieval chains and memory\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "#Misc Libraries\n",
    "import os\n",
    "import json\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5c66c",
   "metadata": {},
   "source": [
    "## 3. Persistent Conversation Memory Class  \n",
    "Stores and loads conversation history to/from disk, ensuring user sessions are persistent across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9cf3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistentConversationMemory:\n",
    "    \"\"\"\n",
    "    A wrapper class around ConversationBufferMemory that adds persistent storage\n",
    "    allowing converstaion history to be saved and retrieved from the disk.\n",
    "    This enables conversation continuity across different application sessions.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath=\"memory.json\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the persistent memory wrapper.\n",
    "        filepath (str): Path tp the JSON file where conversation history will be stored.\n",
    "                        defaults to \"memory.json\"\n",
    "        **kwargs: Additional arguments passed to the underlying ConversationBufferMemory\n",
    "        \"\"\"\n",
    "        self.filepath = filepath #store the filepath for persistent storage\n",
    "        self.memory = ConversationBufferMemory(**kwargs) #create the underlying Langchain memory object\n",
    "        self._load_memory() #Load any existing converstaion history from disk\n",
    "\n",
    "    def _load_memory(self):\n",
    "        \"\"\"\n",
    "        Load the conversation history from the JSON file if it exists.\n",
    "        The JSON file contains a list of message obkects with 'tyoe' and 'content' fields.\n",
    "        Messages are converted back to HumanMessage and AIMessage objects.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.filepath): #check if the path exists\n",
    "            try:\n",
    "                with open(self.filepath,\"r\") as f: #open and parse the JSON file\n",
    "                    data = json.load(f)\n",
    "                self.memory.chat_memory.messages = [ #convert JSON data back to Langchain message objects\n",
    "                    HumanMessage(content=msg[\"content\"]) \n",
    "                    if msg[\"type\"] == \"human\"\n",
    "                    else AIMessage(content=msg[\"content\"])\n",
    "                    for msg in data]\n",
    "                print(f\"[Memory] Loaded {len(self.memory.chat_memory.messages)} messages from the disk\")\n",
    "            except Exception as e: #error handling\n",
    "                print(f\"[Memory] failed to load Memory: {e}\")\n",
    "        else: #no file found\n",
    "            print(\"[Memory] No existing memory file found.\")\n",
    "\n",
    "    def save_memory(self):\n",
    "        \"\"\"\n",
    "        Save the current conversation history to disk on JSON.\n",
    "        Messages are serialized as dictionaries with 'type' amd 'content' fields.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = [ #convert LangChain message object to serializable dictionaries\n",
    "                {\"type\" : \"human\" if isinstance(msg, HumanMessage) \n",
    "                 else \"ai\", \"content\" : msg.content}\n",
    "                 for msg in self.memory.chat_memory.messages]\n",
    "            with open(self.filepath, \"w\") as f: #write the data to JSON file\n",
    "                json.dump(data, f, indent = 2)        \n",
    "            print(f\"[Memory] Saved {len(data)} messages to {self.filepath}\")\n",
    "        except Exception as e: #error handling\n",
    "            print(f\"[Memory] failed to save Memory: {e}\")\n",
    "\n",
    "    def save_context(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Save a new converstaion to memory and then to disk.\n",
    "        This method extends the standard Langchain save_context by \n",
    "        automatically saving the updated conversation history to JSON file.\n",
    "        inputs (dict): the input data (typicaly user query)\n",
    "        outputs (dict): the output data (typically ai response)\n",
    "        \"\"\"\n",
    "        self.memory.save_context(inputs, outputs) #save the underlying context to memory object\n",
    "        self.save_memory() #save it to disk\n",
    "\n",
    "    def load_memory_variables(self, inputs):\n",
    "        \"\"\"\n",
    "        Load memory variables for use in the conversation chain.\n",
    "        This is a passthrough method that calls the underlying memory' load_memory_variables method\n",
    "        inputs (dict): input variables (typicaly empty dict)\n",
    "        return value: memory variables including chat history \n",
    "        \"\"\"\n",
    "        return self.memory.load_memory_variables(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec2792f",
   "metadata": {},
   "source": [
    "## 4. Document Loader & Text Chunking  \n",
    "\n",
    "Load the text file and split it into manageable chunks for effective retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d37dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Path: data/book.txt\n",
      "Split 1 document(s) into 1172 chunks.\n"
     ]
    }
   ],
   "source": [
    "txt_path = \"data/book.txt\"\n",
    "print(f\"text Path: {txt_path}\")\n",
    "# Using TextLoader as example could be replaced with UnstructuredPDFLoader for loading PDF files.\n",
    "loader = TextLoader(txt_path) #intialize the TextLoader to extract text from text file.\n",
    "documents = loader.load() #returns a list of document objects containing the extracted text.\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter( #configure the text splitter for optimal chunk creation.\n",
    "    chunk_size = 1000, #balance the context and retrieval efficiency, lager chunks -> more context may reduce retrieval precision.\n",
    "    chunk_overlap = 500, #overlap to maintain context continuity between chunks.\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]) #define seperators in order for splitting text.\n",
    "\n",
    "chunks = text_splitter.split_documents(documents) #split larger documents into smaller manageable chunks\n",
    "print(f\"Split {len(documents)} document(s) into {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f383b82",
   "metadata": {},
   "source": [
    "## 5. Vector Database Setup\n",
    "\n",
    "Initialize or load persistent vector database with document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ee94244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB path: chromaDB\n",
      "Created and persisted now ChromaDB to chromaDB\n"
     ]
    }
   ],
   "source": [
    "persist_dir = \"chromaDB\"  #define the directory path where ChromaDB will store vector embeddings\n",
    "print(f\"ChromaDB path: {persist_dir}\")\n",
    "\n",
    "#Initialize Ollama embeddings using nomic-embed-text model\n",
    "#this model converts text chunks into high-dementional vectors for semantic search\n",
    "ollama_embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")\n",
    "\n",
    "if os.path.isdir(persist_dir): #Load existing vector database from disk\n",
    "    db = Chroma(persist_directory=persist_dir,\n",
    "                embedding_function=ollama_embeddings)\n",
    "    print(f\"Loaded existing ChromaDB from {persist_dir}\")\n",
    "else: #Create a new vector database from document chunks\n",
    "    #This process involves:\n",
    "    #-1. computing embeddings for each text chunk using embedding model\n",
    "    #-2. store vectors in ChromaDB with associated metadata\n",
    "    #-3. creating search indices for efficient similarity retrieval\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, #text chunks to be embedded\n",
    "        ollama_embeddings, #embedding model\n",
    "        persist_directory=persist_dir) # directory for persistent storage\n",
    "    print(f\"Created and persisted now ChromaDB to {persist_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1c01c",
   "metadata": {},
   "source": [
    "## 6. Language Model & Retrieval Chain Setup  \n",
    "\n",
    "Configure the LLM and the retrieval pipeline for answering queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2b77daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate: Use the following pieces of context to answer the question at the end.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up the answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {input}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a retriever from vector database for semantic document search\n",
    "#this will return the most relevant document chunks based on query similarity\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "#model options for different use cases\n",
    "# - deepseek-r1:14b  (reasoning model)\n",
    "# - gemma3:12b (multi-modal model supporting text and image processing)\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma3:12b\",\n",
    "             callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])) #enable streaming output for realtime response display\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"PromptTemplate: {prompt_template}\")\n",
    "\n",
    "#convert the string template into a langchain PromptTemplate object\n",
    "#enables dynamic variable substitution for context and input\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\",\"input\"])\n",
    "\n",
    "#create a document processing chain:\n",
    "# -  takes retrieved documents and user query\n",
    "# -  combines them using the prompt template\n",
    "combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=PROMPT)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever= retriever, #vecotr database retriever\n",
    "                                        combine_docs_chain=combine_docs_chain) #document processing chain   \n",
    "                                                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a6bc4c",
   "metadata": {},
   "source": [
    "## 7. Persistent Conversation Memory initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069b8da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory] Loaded 8 messages from the disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_6192\\834677291.py:15: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  self.memory = ConversationBufferMemory(**kwargs) #create the underlying Langchain memory object\n"
     ]
    }
   ],
   "source": [
    "memory_file = \"persistent_memory.json\"\n",
    "\n",
    "#initializes persistent memory wrapper class\n",
    "#return_messages = True returns actual HumanMessage and AIMessage objects instead of a single concatenated string\n",
    "memory = PersistentConversationMemory(memory_key = \"chat_history\", #this key will be used when loading converstaion context for the LLM \n",
    "                                      return_messages = True, file_path=memory_file)\n",
    "\n",
    "# memory system behaviour notes:\n",
    "# - on initialization: automatically loads any existing conversation history\n",
    "# - during conversation: maintains context in memory for multi-turn interactions\n",
    "# - After each exchange: automatically saves the updated history to JSON file\n",
    "# - file format: list of messages with 'type' and 'context' fields'\n",
    "# - this enables seamless conversation continuation across application restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42ad57",
   "metadata": {},
   "source": [
    "## 8. Conversational Query Helper\n",
    "\n",
    "Encapsulates the retrieval and memory upate logic for a single question-answer turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b6aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversational_runnable(data):\n",
    "    \"\"\"\n",
    "    Execute a single conversational turn in the RAG system with persistant memory.\n",
    "    This function orchestrates the complete RAG workflow:\n",
    "    - 1. Extracts the user query from input data.\n",
    "    - 2. loads converstaion history from persistent storage.\n",
    "    - 3. Invokes the retrieval chain with context and query.\n",
    "    - 4. saves the new interaction to persistent memory.\n",
    "    - 5. returns the complete response with answer and source documents.\n",
    "\n",
    "    data (dict): Input dictionary containing the user query under \"input\" key.\n",
    "    returns: (dict) complete response containing:\n",
    "        - \"answer\" : the AI - generated response in text.\n",
    "        - \"context\" : list of retrieved document chunks used as sources.\n",
    "        - additional metadata from the retrieval chain.\n",
    "    \"\"\"\n",
    "    question = data[\"input\"] #extract the user's question from the input data structure\n",
    "    \n",
    "    chat_history = memory.load_memory_variables({})[\"chat_history\"] #retrieve the complete conversation history from the persistent memory\n",
    "    \n",
    "    #Use the retriever to find the relevant documnet chunks and combine it with chat history to generate a response using the constructed prompt.\n",
    "    response = retrieval_chain.invoke({\"input\" : question, \"chat_history\" : chat_history})\n",
    "    #save this question-answer pair to persistent memory.\n",
    "    memory.save_context({\"input\" : question}, {\"output\" : response[\"answer\"]})\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c3cb5",
   "metadata": {},
   "source": [
    "## 9. Demo: Run a Sample Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db7f9a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic discussed in this document is Jules Verne's novel, \"Twenty Thousand Leagues Under the Sea.\" It details various aspects of the story, including encounters, geographical locations, and a character's wealth.[Memory] Saved 10 messages to memory.json\n",
      "\n",
      "Query : What is the main topic discussed in this document?\n",
      "AI Response: The main topic discussed in this document is Jules Verne's novel, \"Twenty Thousand Leagues Under the Sea.\" It details various aspects of the story, including encounters, geographical locations, and a character's wealth.\n",
      "\n",
      " Source Documents:\n",
      " - Content: \"One last question, Captain Nemo.\"\n",
      "\"Ask it, Professor.\"\n",
      "\"You are rich?\"\n",
      "\"Immensely rich, sir; and I could, without missing it, pay the national debt of\n",
      "France.\"\n",
      "I stared at the singular person who spo ... \n",
      "   Metadata: {'source': 'data/book.txt'}\n",
      " - Content: clear of all land at a few yards beneath the waves of the Atlantic.\n",
      "CHAPTER XI\n",
      "THE SARGASSO SEA\n",
      "That day the Nautilus crossed a singular part of the Atlantic Ocean. No one\n",
      "can be ignorant of the exist ... \n",
      "   Metadata: {'source': 'data/book.txt'}\n",
      " - Content: CHAPTER II\n",
      "A NOVEL PROPOSAL OF CAPTAIN NEMO'S\n",
      "On the 28th of February, when at noon the Nautilus came to the surface of the\n",
      "sea, in 9Â° 4' N. lat., there was land in sight about eight miles to westwar ... \n",
      "   Metadata: {'source': 'data/book.txt'}\n",
      " - Content: TWENTY THOUSAND LEAGUES\n",
      "UNDER THE SEA\n",
      "by\n",
      "JULES VERNE\n",
      "PART ONE\n",
      "CHAPTER I\n",
      "A SHIFTING REEF\n",
      "The year 1866 was signalised by a remarkable incident, a mysterious and\n",
      "puzzling phenomenon, which doubtless no  ... \n",
      "   Metadata: {'source': 'data/book.txt'}\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main topic discussed in this document?\"\n",
    "response = conversational_runnable({\"input\": query})\n",
    "\n",
    "print(f\"\\nQuery : {query}\")\n",
    "print(f\"AI Response: {response['answer']}\")\n",
    "\n",
    "#optionally print the source documents for context\n",
    "\n",
    "print(\"\\n Source Documents:\")\n",
    "for doc in response['context']:\n",
    "    print(f\" - Content: {doc.page_content[:200]} ... \")\n",
    "    print(f\"   Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92afbc9c",
   "metadata": {},
   "source": [
    "## 10. Next Steps & Improvements\n",
    "\n",
    " - Add a Web UI for interactive querying\n",
    " - Implement multi-document retrieval\n",
    " - Enhance persistent memory structure for user profiles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
